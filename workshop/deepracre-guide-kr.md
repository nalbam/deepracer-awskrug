# 개발자 여러분, 엔진 시동을 거세요!

이 가이드는 RL (강화 학습)의 기본 사항, RL 모델을 훈련하는 방법, 매개 변수로 보상 함수를 정의하는 방법을 안내합니다.

이 지식을 바탕으로 AWS DeepRacer League에서 모델을 훈련하고, 레이스를 할 준비가 될 것입니다.

## 강화 학습이란 무엇입니까?

강화 학습 (RL)은 기계 학습의 한 유형으로, 에이전트는 좋은 결과로 행동을 취하고 나쁜 결과로 행동을 피함으로써 원하는 작업을 수행하는 방법을 배우기 위해 환경을 탐색합니다.

강화 학습 모델은 경험을 통해 학습하고 시간이 지남에 따라 어떤 행동이 최고의 보상으로 이어지는 지 식별 할 수 있습니다.

### 다른 유형의 기계 학습

* 지도 학습
예제 기반 학습 — 주어진 입력에 대해 알려진 출력의 레이블이 지정된 데이터를 사용하여 모델이 새로운 입력에 대한 출력을 예측하도록 학습됩니다.

* 비지도 학습
추론 기반 학습 — 알려진 출력이없는 레이블이 지정되지 않은 데이터를 사용하여 모델이 입력 데이터 내에서 관련 구조 또는 유사한 패턴을 식별하도록 학습됩니다.

## AWS DeepRacer는 스스로 운전하는 법을 어떻게 학습합니까?

강화 학습에서 에이전트는 총 보상을 극대화하려는 목표를 가지고 환경과 상호 작용합니다.

에이전트는 환경 상태에 따라 조치를 취하고 환경은 보상과 다음 상태를 반환합니다. 에이전트는 시행 착오를 통해 학습하며 처음에는 임의의 조치를 취하고 시간이 지남에 따라 장기적인 보상으로 이어지는 조치를 식별합니다.

이러한 아이디어와 AWS DeepRacer와의 관계를 살펴 보겠습니다.

### 에이전트
에이전트는 훈련을 위해 시뮬레이션에서 AWS DeepRacer 차량을 시뮬레이션합니다. 보다 구체적으로, 차량을 제어하고 입력을 받고 행동을 결정하는 신경망을 구현합니다.

### 환경
환경에는 에이전트가 이동할 수있는 위치와 상태를 정의하는 트랙이 포함되어 있습니다. 에이전트는 환경을 탐색하여 데이터를 수집하여 기본 신경망을 훈련시킵니다.

### 상태
상태는 특정 시점에 에이전트가있는 환경의 스냅 샷을 나타냅니다.

AWS DeepRacer의 경우 상태는 차량의 전면 카메라로 캡처 한 이미지입니다.

### 동작
동작은 현재 상태에서 에이전트가 수행 한 이동입니다. AWS DeepRacer의 경우 동작은 특정 속도 및 조향 각도에서의 움직임에 해당합니다.

### 보상
보상은 에이전트가 특정 상태에서 작업을 수행 할 때 피드백으로 제공되는 점수입니다.

AWS DeepRacer 모델을 훈련 할 때 보상은 보상 함수에 의해 반환됩니다. 일반적으로 보상 함수를 정의하거나 제공하여 에이전트가 주어진 상태에서 취해야하는 바람직하거나 바람직하지 않은 조치를 지정합니다.

## 강화 학습 모델을 훈련하는 방법.

### RL 모델 훈련
훈련은 반복적 인 과정입니다. 시뮬레이터에서 에이전트는 환경을 탐색하고 경험을 쌓습니다. 수집 된 경험은 신경망을 주기적으로 업데이트하는 데 사용되며 업데이트 된 모델은 더 많은 경험을 만드는 데 사용됩니다.

AWS DeepRacer를 통해 우리는 스스로 운전할 차량을 훈련하고 있습니다. 훈련 과정을 시각화하는 것은 까다로울 수 있으므로 간단한 예를 살펴 보겠습니다.

### 단순화 된 환경
이 예에서는 차량이 출발점에서 최단 경로를 따라 결승선까지 이동하기를 원합니다.

환경을 정사각형 격자로 단순화했습니다. 각 사각형은 개별 상태를 나타내며 차량이 목표 방향을 향한 상태에서 위아래로 움직일 수 있습니다.

### 점수
그리드의 각 사각형에 점수를 할당하여 인센티브를 제공 할 행동을 결정할 수 있습니다.

여기서 우리는 트랙 가장자리에있는 사각형을 "정지 상태"로 지정하여 차량이 트랙에서 이탈하여 실패했음을 알립니다.

우리는 차량이 트랙 중앙을 주행하는 방법을 배우도록 장려하기 때문에 중앙선에있는 사각형에 대해 높은 보상을 제공하고 다른 곳에서는 낮은 보상을 제공합니다.

### 에피소드
강화 훈련에서 차량은 경계를 벗어나거나 목적지에 도달 할 때까지 그리드를 탐색하여 시작합니다.

주행 할 때 차량은 우리가 정의한 점수로부터 보상을 축적합니다. 이 과정을 에피소드라고합니다.

이 에피소드에서 차량은 정지 상태에 도달하기 전에 2.2의 총 보상을 축적합니다.

### 반복
강화 학습 알고리즘은 누적 보상을 반복적으로 최적화하여 훈련됩니다.

모델은 어떤 행동 (그리고 후속 행동)이 목표에 도달하는 동안 가장 높은 누적 보상을 가져올 것인지 학습합니다.

학습은 처음에만 일어나는 것이 아닙니다. 약간의 반복이 필요합니다. 먼저 에이전트는 지식을 활용하기 전에 가장 높은 보상을받을 수있는 곳을 탐색하고 확인해야합니다.

### 탐사
에이전트가 점점 더 많은 경험을 얻을수록 더 높은 보상을 얻기 위해 중앙의 사각형에 머무르는 법을 배웁니다.

각 에피소드의 총 보상을 플로팅하면 시간이 지남에 따라 모델이 어떻게 작동하고 개선되는지 확인할 수 있습니다.

### 탐색 및 수렴
경험이 많을수록 에이전트가 좋아지고 결국 목적지에 안정적으로 도달 할 수 있습니다.

탐사-탐색 전략에 따라 차량은 환경을 탐색하기 위해 무작위 행동을 취할 가능성이 적을 수 있습니다.

## 보상 기능의 매개 변수.

### AWS DeepRacer에 대한 보상 함수 매개 변수
AWS DeepRacer에서 보상 함수는 현재 상태를 설명하고 숫자 보상 값을 반환하는 특정 매개 변수가 제공되는 Python 함수입니다.

보상 함수에 전달 된 매개 변수는 트랙에서의 위치 및 방향, 관찰 된 속도, 조향 각도 등과 같은 차량 상태의 다양한 측면을 설명합니다.

이러한 매개 변수 중 일부와 트랙을 주행 할 때 차량을 설명하는 방법을 살펴 보겠습니다.

* 트랙에서의 위치
* 차량의 방향
* 웨이 포인트
* 트랙 폭
* 중심선으로부터의 거리
* 궤도상의 모든 바퀴
* 속도
* 조향 각도

1. 트랙에서의 위치
매개 변수 `x` 및 `y` 는 환경의 왼쪽 하단에서 측정 된 차량의 위치를 ​​미터 단위로 설명합니다.

2. 차량의 방향
`heading` 매개 변수는 좌표계의 X 축에서 시계 반대 방향으로 측정 된 차량의 방향을 각도 단위로 설명합니다.

3. 트랙 폭
`track_width` 매개 변수는 미터 단위의 트랙 너비입니다.

4. 웨이 포인트
`waypoints` 매개 변수는 트랙 센터를 따라 배치 된 이정표의 순서 목록입니다.

`waypoints`의 각 웨이 포인트는 차량 위치와 동일한 좌표계에서 측정 된 미터 단위의 좌표 쌍 `[x, y]`입니다.

5. 중심선으로부터의 거리
`distance_from_center` 매개 변수는 트랙 중앙에서 차량의 거리를 측정합니다.

`is_left_of_center` 매개 변수는 차량이 트랙 중앙선의 왼쪽에 있는지 여부를 설명하는 boolean 입니다.

6. 궤도에 모든 바퀴
`all_wheels_on_track` 매개 변수는 boolean (true / false)이며, 차량의 바퀴 네 개가 모두 트랙 경계 안에 있으면 true이고 바퀴가 트랙 밖에 있으면 false입니다.

7. 속도
`speed` 매개 변수는 초당 미터 단위로 측정 된 차량의 관측 된 속도를 측정합니다.

8. 조향 각도
`steering_angle` 매개 변수는도 단위로 측정 된 차량의 스티어링 각도를 측정합니다.

이 값은 차량이 오른쪽으로 조향하면 음수이고 차량이 왼쪽으로 조향하면 양수입니다.

9. 요약
총 13 개의 매개 변수를 보상 기능에 사용할 수 있습니다.

| 변수 | 설명 |
| ---- | --- |
| x , y | 트랙에서 차량의 위치 |
| heading | 트랙에서 차량의 방향 |
| waypoints | 경유지 좌표 목록 |
| closest_waypoints | 차량에 가장 가까운 두 웨이 포인트의 인덱스 |
| progress | 완료된 트랙의 백분율 |
| steps | 완료된 단계 수 |
| track_width | 트랙의 너비 |
| distance_from_center | 트랙 중심선으로부터의 거리 |
| is_left_of_center | 차량이 중앙선 왼쪽에 있는지 여부 |
| all_wheels_on_track | 차량이 완전히 트랙 경계 내에 있습니까? |
| speed | 차량의 관측 속도 |
| steering_angle | 앞바퀴의 스티어링 각도 |

## 보상 기능.

### 함께 모아서
이러한 모든 매개 변수를 마음대로 사용하면 원하는 운전 행동을 장려하는 보상 함수를 정의 할 수 있습니다.

보상 함수의 몇 가지 예와 매개 변수를 사용하여 보상을 결정하는 방법을 살펴 보겠습니다. 다음 세 가지 보상 함수는 AWS DeepRacer 콘솔에서 예제로 사용할 수 있으므로이를 사용해보고 작동 방식을 확인하거나 AWS DeepRacer League에 제출할 수 있습니다.

1. 궤도 유지

이 예에서 우리는 자동차가 트랙에 머무를 때 높은 보상을주고, 자동차가 트랙 경계를 벗어나면 페널티를줍니다.

이 예에서는 `all_wheels_on_track`, `distance_from_center` 및 `track_width` 매개 변수를 사용하여 자동차가 트랙에 있는지 여부를 확인하고 높은 보상을 제공합니다.

이 기능은 트랙에 머무르는 것 외에 특정 종류의 행동을 보상하지 않기 때문에이 기능으로 훈련 된 에이전트는 특정 행동에 수렴하는 데 더 오랜 시간이 걸릴 수 있습니다.

2. 중심선 따라 가기

이 예에서는 자동차가 트랙 중앙에서 얼마나 멀리 떨어져 있는지 측정하고 자동차가 중앙선에 가까우면 더 높은 보상을 제공합니다.

이 예에서는 `track_width` 및 `distance_from_center` 매개 변수를 사용하고, 자동차가 트랙의 중심에서 멀어 질수록 감소하는 보상을 반환합니다.

이 예는 보상 할 운전 행동의 종류에 대해 더 구체적이므로이 기능으로 훈련 된 에이전트는 트랙을 잘 따라가는 법을 배울 가능성이 높습니다. 그러나 코너에서 가속 또는 제동과 같은 다른 동작은 배우지 않을 것입니다.

3. 인센티브 없음

대안 전략은 자동차가 어떻게 운전하고 있는지에 관계없이 각 단계마다 지속적인 보상을 제공하는 것입니다.

이 예제에서는 입력 매개 변수를 사용하지 않고 대신 각 단계에서 1.0의 상수 보상을 반환합니다.

에이전트의 유일한 인센티브는 트랙을 성공적으로 완료하는 것이며, 더 빨리 운전하거나 특정 경로를 따라갈 인센티브가 없습니다. 비정상적으로 작동 할 수 있습니다.

그러나 보상 기능은 에이전트의 행동을 제한하지 않기 때문에 잘 수행되는 것으로 판명 된 예상치 못한 전략과 행동을 탐색 할 수 있습니다.

## 축하합니다!

고무가 도로를 칠 때입니다. 보상 기능이 랩 타임을 개선하는 방법을 배웠으므로 이제 AWS DeepRacer League에 참가할 준비가되었습니다.
